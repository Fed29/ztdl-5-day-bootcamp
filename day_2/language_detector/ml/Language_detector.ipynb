{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The training data folder must be passed as first argument\n",
    "try:\n",
    "  dataset = load_files('./wikidata/short_paragraphs')\n",
    "except OSError as ex:\n",
    "  print(ex)\n",
    "  print(\"Couldn't import the data, did you unzip the wikidata.zip folder?\")\n",
    "  exit(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = dataset.data\n",
    "labels = dataset.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'par les diff\\xc3\\xa9rentes communaut\\xc3\\xa9s de langue,',\n",
       " b\"l'episodio. I suoi redattori hanno\",\n",
       " b'especie de legitimidad democr\\xc3\\xa1tica. Por',\n",
       " b'Britannica; sin embargo, ninguna ha',\n",
       " b'el mundo,[4] y pr\\xc3\\xa1cticamente cualquier']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 3, 3, 3])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Split the dataset in training and test set\n",
    "# Use 20% of the data for test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_train, docs_test, y_train, y_test = train_test_split(docs, labels, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Build a an vectorizer that splits\n",
    "# strings into sequence of 1 to 3\n",
    "# characters instead of word tokens\n",
    "# using the class TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = TfidfVectorizer(ngram_range=(1,3), analyzer=\"char\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Use the function make_pipeline to build a\n",
    "#       vectorizer / classifier pipeline\n",
    "#       using the previous analyzer\n",
    "#       and a classifier of choice.\n",
    "#       The pipeline instance should be\n",
    "#       stored in a variable named model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<5x55115 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 19 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WITHOUT A PIPELINE IT WILL BE\n",
    "vec.fit(docs_train)\n",
    "vec.transform(docs_test[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH A PIPELINE WE WILL CREATE A PIPELINE (A BLOCK) WITH [VECTORIZER + MODEL ESTIMATOR] WHICH RECEIVE PHRASES AND RETURN OUTPUTS\n",
    "from sklearn.pipeline import make_pipeline\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "model = make_pipeline(vec, clf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('tfidfvectorizer', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_i...n_jobs=1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False))])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TASK: Fit the pipeline on the training set\n",
    "model.fit(docs_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Predict the outcome on the testing set.\n",
    "# Store the result in a variable named y_predicted\n",
    "\n",
    "y_predicted = model.predict(docs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      0.60      0.75        62\n",
      "          1       0.93      0.91      0.92       193\n",
      "          2       0.93      0.91      0.92       221\n",
      "          3       0.79      0.80      0.80       213\n",
      "          4       0.91      0.83      0.87       223\n",
      "          5       0.83      0.81      0.82       198\n",
      "          6       0.91      0.55      0.69        78\n",
      "          7       0.90      0.66      0.76       112\n",
      "          8       0.81      0.86      0.83       207\n",
      "          9       0.62      0.98      0.76       185\n",
      "\n",
      "avg / total       0.85      0.83      0.83      1692\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TASK: Print the classification report\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TASK: Print the confusion matrix. Bonus points if you make it pretty.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    p_ar  p_de  p_en  p_es  p_fr  p_it  p_nl  p_pl  p_pt  p_ru\n",
      "ar    37     1     0     0     0     0     2     0     0    22\n",
      "de     0   176     0     2     3     5     0     0     1     6\n",
      "en     0     3   202     0     2     6     2     2     0     4\n",
      "es     0     1     0   170     1     8     0     0    21    12\n",
      "fr     0     0     1    18   184     4     0     0     9     7\n",
      "it     0     3     1     6     8   161     0     0     4    15\n",
      "nl     0     4     6    12     0     7    43     1     2     3\n",
      "pl     0     1     2     0     1     1     0    74     3    30\n",
      "pt     0     0     3     6     3     1     0     5   177    12\n",
      "ru     0     0     3     0     0     0     0     0     1   181\n"
     ]
    }
   ],
   "source": [
    "print(pd.DataFrame(cm, index=dataset.target_names, columns=[\"p_\" + c for c in dataset.target_names]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze (vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(1, 3)), classifier = RandomForestClassifier()) :\n",
    "    \"\"\"Build a language detector model\n",
    "\n",
    "    The goal of this exercise is to train a linear classifier on text features\n",
    "    that represent sequences of up to N consecutive characters so as to be\n",
    "    recognize natural languages by using the frequencies of short character\n",
    "    sequences as 'fingerprints'.\n",
    "\n",
    "    The script saves the trained model to disk for later use\n",
    "    \"\"\"\n",
    "    # TASK: Split the dataset in training and test set\n",
    "    # (use 20% of the data for test):\n",
    "    docs_train, docs_test, y_train, y_test = \\\n",
    "        train_test_split(docs, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "    # TASK: Build a an vectorizer that splits\n",
    "    # strings into sequence of 1 to 3\n",
    "    # characters instead of word tokens\n",
    "    # using the class TfidfVectorizer\n",
    "    vec = vectorizer\n",
    "\n",
    "\n",
    "    # TASK: Use the function make_pipeline to build a\n",
    "    #       vectorizer / classifier pipeline\n",
    "    #       using the previous analyzer\n",
    "    #       and a classifier of choice.\n",
    "    #       The pipeline instance should be\n",
    "    #       stored in a variable named model\n",
    "    clf = classifier\n",
    "    model = make_pipeline(vec, clf)\n",
    "\n",
    "    # TASK: Fit the pipeline on the training set\n",
    "    model.fit(docs_train, y_train)\n",
    "\n",
    "    # TASK: Predict the outcome on the testing set.\n",
    "    # Store the result in a variable named y_predicted\n",
    "    y_predicted = model.predict(docs_test)\n",
    "\n",
    "    # TASK: Print the classification report\n",
    "    print(classification_report(y_test, y_predicted))\n",
    "\n",
    "    # TASK: Print the confusion matrix. Bonus points if you make it pretty.\n",
    "    cm = confusion_matrix(y_test, y_predicted)\n",
    "\n",
    "    idx = dataset.target_names\n",
    "    cols = ['p_'+c for c in dataset.target_names]\n",
    "    print(pd.DataFrame(cm, index=idx, columns=cols))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        62\n",
      "          1       0.97      0.98      0.98       193\n",
      "          2       0.93      0.99      0.96       221\n",
      "          3       0.94      0.94      0.94       213\n",
      "          4       0.97      0.99      0.98       223\n",
      "          5       0.97      0.97      0.97       198\n",
      "          6       1.00      0.83      0.91        78\n",
      "          7       1.00      0.97      0.99       112\n",
      "          8       0.95      0.94      0.95       207\n",
      "          9       1.00      0.98      0.99       185\n",
      "\n",
      "avg / total       0.97      0.97      0.97      1692\n",
      "\n",
      "    p_ar  p_de  p_en  p_es  p_fr  p_it  p_nl  p_pl  p_pt  p_ru\n",
      "ar    62     0     0     0     0     0     0     0     0     0\n",
      "de     0   190     2     1     0     0     0     0     0     0\n",
      "en     0     1   219     0     1     0     0     0     0     0\n",
      "es     0     0     0   201     0     4     0     0     8     0\n",
      "fr     0     0     1     1   221     0     0     0     0     0\n",
      "it     0     1     2     2     0   192     0     0     1     0\n",
      "nl     0     4     7     1     1     0    65     0     0     0\n",
      "pl     0     0     1     0     1     0     0   109     1     0\n",
      "pt     0     0     2     7     3     0     0     0   195     0\n",
      "ru     0     0     2     0     0     1     0     0     0   182\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        62\n",
      "          1       0.99      0.98      0.99       193\n",
      "          2       0.94      0.99      0.97       221\n",
      "          3       0.95      0.95      0.95       213\n",
      "          4       0.98      0.99      0.99       223\n",
      "          5       0.97      0.96      0.97       198\n",
      "          6       1.00      0.95      0.97        78\n",
      "          7       1.00      0.99      1.00       112\n",
      "          8       0.95      0.95      0.95       207\n",
      "          9       1.00      0.98      0.99       185\n",
      "\n",
      "avg / total       0.97      0.97      0.97      1692\n",
      "\n",
      "    p_ar  p_de  p_en  p_es  p_fr  p_it  p_nl  p_pl  p_pt  p_ru\n",
      "ar    62     0     0     0     0     0     0     0     0     0\n",
      "de     0   190     2     1     0     0     0     0     0     0\n",
      "en     0     1   219     0     0     0     0     0     1     0\n",
      "es     0     0     0   202     0     4     0     0     7     0\n",
      "fr     0     0     1     1   221     0     0     0     0     0\n",
      "it     0     0     3     2     1   191     0     0     1     0\n",
      "nl     0     0     2     1     1     0    74     0     0     0\n",
      "pl     0     0     0     0     0     1     0   111     0     0\n",
      "pt     0     0     3     5     2     0     0     0   197     0\n",
      "ru     0     0     2     0     0     0     0     0     1   182\n"
     ]
    }
   ],
   "source": [
    "vectorizers = [\n",
    "#     TfidfVectorizer(analyzer='char', ngram_range=(1, 3)),\n",
    "#     TfidfVectorizer(analyzer='char', ngram_range=(2, 3)),\n",
    "    TfidfVectorizer(analyzer='char', ngram_range=(2, 4)),\n",
    "#     TfidfVectorizer(analyzer='char', ngram_range=(1, 4))\n",
    "]\n",
    "\n",
    "models = [\n",
    "#     RandomForestClassifier(max_depth=20),\n",
    "#     DecisionTreeClassifier(max_depth=20),\n",
    "#     MLPClassifier(),\n",
    "    LogisticRegression(),\n",
    "    LogisticRegression(C=10)\n",
    "]\n",
    "\n",
    "for vectorizer in vectorizers:\n",
    "    for model in models:\n",
    "        analyze(vectorizer=vectorizer, classifier=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
